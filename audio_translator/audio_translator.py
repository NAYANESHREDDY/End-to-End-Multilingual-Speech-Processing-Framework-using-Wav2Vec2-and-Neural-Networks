# -*- coding: utf-8 -*-
"""audio_translator.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FHfuyqlwFX0B4hvT3tb4mS7jG3L-xheG
"""

# --- Step 1: Install all the necessary libraries (with CORS support) ---
!pip install -q openai-whisper google-generativeai moviepy pydub transformers torch soundfile flask pyngrok flask-cors

print("‚úÖ Libraries installed.")

# --- Step 2: Import everything and define functions ---
import os
import tempfile
import torch
import whisper
from moviepy.editor import VideoFileClip
from pydub import AudioSegment
from transformers import VitsModel, VitsTokenizer
import soundfile as sf
import google.generativeai as genai
from google.colab import userdata
from flask import Flask, request, jsonify
from flask_cors import CORS # Import CORS
from pyngrok import ngrok
import werkzeug
import uuid # To create unique session IDs
import base64 # To encode the audio for sending via JSON

# --- Gemini API Key Configuration ---
try:
    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')
    genai.configure(api_key=GEMINI_API_KEY)
except Exception as e:
    print("‚ùå Could not configure Gemini API key. Please add it to Colab Secrets.")

# --- Configuration & Functions ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MMS_LANGUAGE_MAP = { 'as': 'asm', 'bn': 'ben', 'gu': 'guj', 'hi': 'hin', 'kn': 'kan', 'ks': 'kas', 'mai': 'mai', 'ml': 'mal', 'mni': 'mni', 'mr': 'mar', 'ne': 'nep', 'or': 'ory', 'pa': 'pan', 'sa': 'san', 'sat': 'sat', 'sd': 'snd', 'ta': 'tam', 'te': 'tel', 'ur': 'urd' }
ALL_INDIAN_LANGUAGES = { "Assamese": "as", "Bengali": "bn", "Gujarati": "gu", "Hindi": "hi", "Kannada": "kn", "Kashmiri": "ks", "Maithili": "mai", "Malayalam": "ml", "Manipuri": "mni", "Marathi": "mr", "Nepali": "ne", "Odia": "or", "Punjabi": "pa", "Sanskrit": "sa", "Santali": "sat", "Sindhi": "sd", "Tamil": "ta", "Telugu": "te", "Urdu": "ur" }
SUPPORTED_TTS_LANGUAGES = set(MMS_LANGUAGE_MAP.keys())
INDIAN_LANGUAGES = { name: code for name, code in ALL_INDIAN_LANGUAGES.items() if code in SUPPORTED_TTS_LANGUAGES }

# --- (Paste ALL your original helper functions here without changes) ---
# extract_audio_from_video, standardize_audio, transcribe_audio, etc.
def extract_audio_from_video(video_path: str) -> str:
    if not os.path.exists(video_path): raise FileNotFoundError(f"Video file not found at {video_path}")
    video_clip = VideoFileClip(video_path)
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
    video_clip.audio.write_audiofile(temp_file, codec='pcm_s16le', logger=None)
    video_clip.close()
    return temp_file

def standardize_audio(input_path: str, target_sr: int = 16000) -> str:
    audio = AudioSegment.from_file(input_path)
    audio = audio.set_frame_rate(target_sr).set_channels(1)
    standardized_file = tempfile.NamedTemporaryFile(delete=False, suffix="_standardized.wav").name
    audio.export(standardized_file, format="wav")
    return standardized_file

def transcribe_audio(audio_path: str) -> dict:
    model = whisper.load_model("medium")
    result = model.transcribe(audio_path, fp16=torch.cuda.is_available())
    return result

def translate_text_with_gemini(text: str, source_lang: str, target_lang_name: str) -> str:
    model = genai.GenerativeModel('gemini-pro-latest')
    prompt = (f"Translate the following text from {source_lang} to {target_lang_name}. "
              f"Your translation should be accurate. Text to translate: '{text}'")
    response = model.generate_content(prompt)
    return response.text.strip()

def synthesize_with_mms(text: str, target_lang_code: str, output_path: str):
    mms_code = MMS_LANGUAGE_MAP.get(target_lang_code)
    if not mms_code: raise ValueError(f"No MMS model found for lang code: {target_lang_code}")
    model_id = f"facebook/mms-tts-{mms_code}"
    model = VitsModel.from_pretrained(model_id).to(DEVICE)
    tokenizer = VitsTokenizer.from_pretrained(model_id)
    inputs = tokenizer(text, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        output = model(**inputs).waveform
    sample_rate = model.config.sampling_rate
    sf.write(output_path, output.cpu().numpy().squeeze(), samplerate=sample_rate)

app = Flask(__name__)
CORS(app) # Enable CORS for all routes

UPLOAD_FOLDER = 'temp_uploads'
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

# A temporary dictionary to store session data between requests
SESSIONS = {}

@app.route("/")
def home():
    return "<h1>Translation API is Running!</h1><p>Ready for two-step translation.</p>"

@app.route('/upload', methods=['POST'])
def upload_and_transcribe():
    try:
        if 'file' not in request.files:
            return jsonify({"error": "No file part"}), 400
        file = request.files['file']
        if file.filename == '':
            return jsonify({"error": "No selected file"}), 400

        filename = werkzeug.utils.secure_filename(file.filename)
        input_path = os.path.join(UPLOAD_FOLDER, filename)
        file.save(input_path)

        video_extensions = ['.mp4', '.mov', '.avi', '.mkv']
        if any(input_path.lower().endswith(ext) for ext in video_extensions):
            raw_audio_path = extract_audio_from_video(input_path)
        else:
            raw_audio_path = input_path

        standardized_audio = standardize_audio(raw_audio_path)

        transcription_result = transcribe_audio(standardized_audio)
        source_text = transcription_result['text']
        source_lang = transcription_result['language']

        # Create a unique session ID to track this request
        session_id = str(uuid.uuid4())
        # Store the necessary data for the next step
        SESSIONS[session_id] = {
            "transcribed_text": source_text,
            "source_lang": source_lang
        }

        # Clean up the initial files
        if raw_audio_path != input_path: os.remove(raw_audio_path)
        os.remove(standardized_audio)
        os.remove(input_path)

        return jsonify({
            "session_id": session_id,
            "transcribed_text": source_text,
            "languages": INDIAN_LANGUAGES
        })

    except Exception as e:
        print(f"Error in /upload: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/translate', methods=['POST'])
def translate_and_synthesize():
    try:
        data = request.get_json()
        session_id = data.get('session_id')
        target_lang_code = data.get('target_lang_code')

        if not session_id or session_id not in SESSIONS:
            return jsonify({"error": "Invalid or expired session"}), 400

        session_data = SESSIONS[session_id]
        target_lang_name = [name for name, code in INDIAN_LANGUAGES.items() if code == target_lang_code][0]

        translated_text = translate_text_with_gemini(
            session_data['transcribed_text'],
            session_data['source_lang'],
            target_lang_name
        )

        output_filename = os.path.join(UPLOAD_FOLDER, f"{session_id}.wav")
        synthesize_with_mms(translated_text, target_lang_code, output_filename)

        # Read the audio file and encode it in Base64
        with open(output_filename, "rb") as audio_file:
            encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')

        # Clean up the session and the final audio file
        del SESSIONS[session_id]
        os.remove(output_filename)

        return jsonify({"audio_data": encoded_audio, "translated_text": translated_text})

    except Exception as e:
        print(f"Error in /translate: {e}")
        return jsonify({"error": str(e)}), 500

# --- Step 4: Expose the API with ngrok and Run the Server ---
try:
    NGROK_TOKEN = userdata.get('NGROK_AUTHTOKEN')
    ngrok.set_auth_token(NGROK_TOKEN)
    public_url = ngrok.connect(5000)
    print("="*50)
    print(f"üöÄ Your API is live at: {public_url}")
    print("="*50)
    app.run(port=5000)
except Exception as e:
    print(f"‚ùå Could not start ngrok. Please add your NGROK_AUTHTOKEN to Colab Secrets.")